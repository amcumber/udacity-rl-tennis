[environment]
# ENV_FILE = 'envs/Reacher_Windows_x86_64-twenty-agents/Reacher.exe'
ENV_FILE = ''
ENV_TYPE = 'tennis'        # enum ('unity', 'gym') = choose which environment to run
CLOUD = true               # True if running in Udacity venv
STATE_SIZE = 24            # State size for given environment
ACTION_SIZE = 2            # Action size for given environment
UPPER_BOUND = 1.0          # upper bound of action space

[trainer]
BATCH_SIZE = 16            # minibatch size
N_EPISODES = 1000          # max number of episodes to run
MAX_T = inf                # Max time steps within an episode
WINDOW_LEN = 100           # window length for averaging
SAVE_ALL = true            # Save meta data from trainer as a pickle file`

[agent]
N_AGENTS = 2
BUFFER_SIZE = 1000000      # Replay buffer size
LEARN_F = 20               # Learning Frequency within episodes
GAMMA = 0.99               # discount factor
TAU = 1e-3                 # soft update target parameter
LR_ACTOR = 1e-4            # learning rate for the actor
LR_CRITIC = 1e-4           # learning rate for the critic
WEIGHT_DECAY = 0.0         # (0.0001) L2 weight decay parameter
ACTOR_HIDDEN = [64, 64]    # Actor Hidden architecture s -> h1 -> ... -> hn -> a
CRITIC_HIDDEN = [64, 64]   # Critic s -> h1+a -> ... -> hn -> 1
ACTOR_ACT = 'relu'         # Actor Activation Function
CRITIC_ACT = 'leaky_relu'  # Actor Activation Function
ADD_NOISE = [false, false] # [false | "OU" | "Stocastic" ] add noise to the agent's action
